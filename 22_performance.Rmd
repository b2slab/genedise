---
title: "Benchmark of network-based methods on all the diseases"
author: "Sergio Picart-Armada"
date: "December 28, 2017"
output: html_document
---

```{r setup, include=FALSE}
# Show the code, but suppress messages and warnings
# otherwise document gets too long
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

# Getting started

## Load libraries and data

```{r}
# Data handling
library(plyr)
library(dplyr)
library(tidyr)
library(magrittr)

# ML libraries
library(kernlab)
library(caret)
library(mlr)
library(parallelMap)

# other
library(igraph)
library(ggplot2)
library(ggsci)
library(diffuStats)

# state of the art methods
library(EGAD)
library(RANKS)
library(COSNet)

# for models
library(lsmeans)
library(multcomp)

# have all config variables in a different env
config <- new.env(parent = globalenv())
source("20_config.R", local = config)

# Parallel backend
parallelStartSocket(config$nslaves)

# load dataset and kernel (heavy, 1Gb)
load(config$graph_alldiseases)
load(config$file_kernel)

# adjacency matrix (heavy, 1Gb)
A <- igraph::as_adj(g_filter, sparse = TRUE, attr = "weight") %>% as.matrix

# Load MashUp features
nm_mashup <- readLines(config$file_mashup_names)
df_mashup <- data.table::fread(config$file_mashup_features) %>% 
  t %>% 
  as.data.frame %>% 
  set_rownames(nm_mashup)

# the dataset from OpenTargets
df_disease <- mutate(
  g_filter$dataset, 
  disease = as.factor(disease.efo_info.label)) %>% 
  rename(drugs = known_drug_binary, genetic = known_gene_binary) %>% 
  mutate(validation = drugs)
# complex data
df_complex <- g_filter$df_complex
list_complex <- g_filter$list_complex

n_total <- vcount(g_filter)
```

## Input data 

```{r}
# data frame with input scores in long format
# use drugs and genetic as input streams
df_input <- plyr::ddply(
  reshape2::melt(
    df_disease, measure.vars = c("drugs", "genetic"), 
    variable.name = "input_type", value.name = "input"),
  c("disease", "input_type"), 
  function(disease) {
    # browser()
    nm <- V(g_filter)$name
    x <- nm %in% (filter(disease, input == 1)$STRING_id)
    val <- nm %in% (filter(disease, validation == 1)$STRING_id)
    
    data.frame(
      STRING_id = nm, 
      score = x*1, 
      validation = val*1
    )
  }, 
  .progress = "text"
)

# Save the drugs data as a long data frame
df_drugs <- filter(df_input, input_type == "drugs") %>%
  dplyr::select(disease, STRING_id, validation) %>% 
  rename(drugs = validation)
# pairs disease-gene (drugs == 0 not included to save space)
write.csv(filter(df_drugs, drugs == 1L) %>% dplyr::select(-drugs), 
          file = paste0(config$dir_performance, "/data_disease_drugs.csv"), 
          row.names = FALSE)

# count in how many diseases genes participate
# Only write genes in many diseases
df_ndisease <- plyr::ddply(
  df_drugs, "STRING_id", 
  function(df) c(n_disease = sum(df$drugs))) %>% 
  dplyr::filter(n_disease >= 10) %>%
  dplyr::arrange(desc(n_disease))
write.csv(df_ndisease, 
          file = paste0(config$dir_performance, "/descriptive_disease_count_genes.csv"), 
          row.names = FALSE)

# Small tests
local({
  a <- subset(df_input, input_type == "drugs")
  b <- subset(df_input, input_type == "genetic")
  tr <- all(a$score == a$validation) & all(a$score == b$validation)  
  stopifnot(tr)
})

# select streams other than drug and overall
# to use them as "reference streams" in the benchmark
# data frame in long format
df_streams <- dplyr::select(
  df_disease, disease, STRING_id, 
  contains("association_score"), -contains("drug"), 
  -contains("overall")) %>% 
  gather(key = stream, value = score, 
         contains("association_score"), factor_key = TRUE) %>%
  ddply(
    c("disease", "stream"), 
    function(df_dis) {
      nm <- V(g_filter)$name
      x <- nm %in% df_dis$STRING_id
      
      original <- setNames(numeric(length(nm)), nm)
      original[as.character(df_dis$STRING_id)] <- df_dis$score
      
      data.frame(STRING_id = nm, score = original)
    }, 
    .progress = "text"
)
```

```{r}
# Descriptive data about the diseases: 
# number of genetic genes, number of drugs genes, 
# overlap (with p-value and fdr)
df_descriptive <- plyr::ddply(
  dplyr::select(df_input, -validation) %>% 
    reshape2::dcast(STRING_id+disease~input_type, 
                    fun.aggregate = NULL, value.var = "score"), 
  "disease", 
  function(df) {
    tibble(
      n_genetic = sum(df$genetic), 
      n_drug = sum(df$drugs), 
      overlap = sum(df$genetic*df$drugs), 
      p_value = fisher.test(x = df$genetic, y = df$drugs)$p.value
    ) 
  }, 
  .id = "disease") %>% 
  mutate(fdr = p.adjust(p_value, method = "fdr"))

write.csv(df_descriptive, 
          file = paste0(config$dir_performance, "/descriptive_diseases.csv"), 
          row.names = FALSE)

# check for NAs in the input
stopifnot(all(!is.na(df_input)))

# sanity check: all diseases must contain 1's but have a mean closer to 0
# (most genes are negatives)
group_by(df_input, disease, input_type) %>%
  dplyr::select(score) %>%
  summarise_all(c("min", "mean", "max"))
```

# Define functions

## Performance measures

```{r}
# define function top_k to count number of 1s in the 
# top k entities of a list. 
# It is assumed that largest scores are best
top_k <- function(k) {
  function(actual, predicted) {
    inds <- head(order(predicted, decreasing = TRUE), k)
    sum(actual[inds])
  }
}

# define a list of metric functions to iterate over
# partial aurocs are normalised between 0 and 1
list_metrics <- list(
  auroc = metric_fun(curve = "ROC"), 
  partial_auroc_0.90 = metric_fun(curve = "ROC", c(0, .1), standardized = TRUE), 
  partial_auroc_0.95 = metric_fun(curve = "ROC", c(0, .05), standardized = TRUE), 
  auprc = metric_fun(curve = "PRC"),
  top_20_hits = top_k(20), 
  top_100_hits = top_k(100)
) 
```

## Functions for ML-based methods

```{r}
# C-SVM Bagging wrapper 
# Assumption: number of positives << number of negatives 
# Aggregation is at the level of "decision" values!
# Because if it is on the predicted class, it 
# does not work well, i.e. too many times the SVM 
# predicts only positives (or negatives), although the
# raking of the "decision" values can be meaningful
# (bootstrap is on the negatives!)
# ind_train, ind_test: numeric or character vectors with the 
# ids of the training and the testing samples
# ytrain binary vector with training labels (+:1, -:0)
# K graph kernel matrix
# B number of bootstrapping iterations
# ... further arguments for ksvm
bag_svm <- function(ind_train, ind_test, K, ytrain, B = 30, ...) {
  stopifnot(length(ind_train) == length(ytrain))
  
  # SVM bagging
  ind_pos <- which(ytrain == 1L)
  ind_neg <- which(ytrain == 0L)
  npos <- length(ind_pos)
  nneg <- length(ind_neg)
  
  yt <- as.factor(ytrain)
  mat_bag <- plyr::ldply(
    1:B, 
    function(rep) {
      # browser()
      ind_bag_neg <- sample(ind_neg, npos, replace = TRUE)
      ind_bag_all <- c(ind_pos, ind_bag_neg)
      # this one can contain names:
      ind_bag_orig <- ind_train[ind_bag_all]
      
      # train svms
      svm_mod <- ksvm(
        as.kernelMatrix(K[ind_bag_orig, ind_bag_orig]), 
        yt[ind_bag_all], 
        kernel = "matrix", 
        ...
      )
      # find support vectors
      svm_vec <- ind_bag_orig[SVindex(svm_mod)]
      
      # predict using precomputed kernel
      predict(svm_mod, 
              as.kernelMatrix(K[ind_test, svm_vec]), 
              type = "decision") %>% as.vector
    }, 
    .progress = "text"
    # this last line will work if ind_test are rownames and if they are 
    # numeric values
  ) %>% colMeans %>% setNames(rownames(K[ind_test, 1, drop = FALSE]))
}

# nu-SVM RBF and random forest wrapper, with data downsampling
# Assumption: number of positives << number of negatives 
# ind_train, ind_test: numeric or character vectors with the 
# ids of the training and the testing samples
# ytrain binary vector with training labels (+:1, -:0)
# df_features MashUp features as a data frame
mlr_svm_rf <- function(ind_train, ind_test, df_features, ytrain) {
  # levels are 0, 1 (in this order)
  yclass <- as.factor(ytrain)
  mean_y <- mean(ytrain)
  # how many more negatives are there?
  ratio <- (1 - mean_y)/mean_y
  
  # create task
  # shared by all learners
  # positives are coded as "1" and negatives as "0"
  tsk <- makeClassifTask(
    id = "diffu", 
    data = data.frame(class = yclass, df_features[ind_train, ]), 
    target = "class", 
    positive = "1")
  tsk <- undersample(tsk, 1/ratio)

  ###### SVM LEARNER ###### 
  num_ps <- makeParamSet(
    # makeNumericParam("C", lower = -5, upper = 5, trafo = function(x) 10^x),
    makeNumericParam("nu", lower = .1, upper = .9),
    makeNumericParam("sigma", lower = -6, upper = 2, trafo = function(x) 10^x)
  )
  ctrl <- makeTuneControlGrid(resolution = 5L, tune.threshold = FALSE)
  
  # define learner
  lrn <- makeLearner("classif.ksvm", predict.type = "prob")
  rdesc <- makeResampleDesc("CV", iters = 3L, stratify = TRUE)
  
  # Grid search in parallel
  res <- tuneParams(
    lrn, 
    task = tsk, 
    resampling = rdesc,
    par.set = num_ps, 
    measures = list(auc),
    control = ctrl)
  
  # Fit optimal params
  lrn.optim <- setHyperPars(lrn, par.vals = res$x)
  m <- train(lrn.optim, tsk)
  m
  
  # predict
  pred_svm <- predict(m, newdata = df_features[ind_test, ])
  
  ###### RandomForest LEARNER ###### 
  num_ps <- makeParamSet(
    makeIntegerParam("ntree", lower = 10, upper = 500), 
    # makeIntegerParam("mtry", lower = 10, upper = 50), 
    makeIntegerParam("nodesize", lower = 1, upper = 5)
  )
  ctrl <- makeTuneControlGrid(resolution = 3L, tune.threshold = TRUE)
  
  # define learner
  lrn <- makeLearner("classif.randomForest", predict.type = "prob")
  rdesc <- makeResampleDesc("CV", iters = 3L, stratify = TRUE)
  
  # Grid search in parallel
  res <- tuneParams(
    lrn, 
    task = tsk, 
    resampling = rdesc,
    par.set = num_ps, 
    measures = list(auc),
    control = ctrl)
  
  # Fit optimal params
  lrn.optim <- setHyperPars(lrn, par.vals = res$x)
  m <- train(lrn.optim, tsk)
  m
  
  # Test set
  pred_rf <- predict(m, newdata = df_features[ind_test, ])
  
  list(
    svm = setNames(pred_svm$data$prob.1, rownames(pred_svm$data)), 
    rf = setNames(pred_rf$data$prob.1, rownames(pred_rf$data))
  )
}
```


## PageRank as a baseline centrality measure

```{r}
# reproducibility
set.seed(1)

# A centrality measure
pr <- page.rank(g_filter)$vector
```

# Complex-aware cross validation

## Precompute cross validation folds

```{r}
set.seed(1)

# list with validation folds
# the format is list$disease$cv_scheme${train,validation}
# train and validation contain the indices of the genes that 
# should be used for both purposes. 
# If validation is NULL, then the complementary of train should be used
list_cv_folds <- plyr::dlply(
  # filter(df_input, disease %in% c("allergy", "chronic obstructive pulmonary disease")), 
  filter(df_input, input_type == "drugs"),
  "disease", 
  function(df_in) {
    # browser()
    ans <- list()
    
    name_disease <- as.character(df_in$disease[1])

    # take response only
    y <- setNames(df_in$validation, df_in$STRING_id)
    
    #######################################
    # First approach: classic stratified CV
    folds_classic <- caret::createMultiFolds(
      y = as.factor(y), 
      k = config$k_cv, 
      times = config$times_cv)
    # Rename to be consistent with other schemes
    names_classic <- names(folds_classic) %>% 
      strsplit(split = "\\.") %>%  
      sapply(function(x) paste(rev(x), collapse = "."))  
    names(folds_classic) <- names_classic
    
    ans$classic <- lapply(
      folds_classic, 
      function(x) list(train = x)
    )
    
    #######################################
    # Complex-aware approaches
     
    # First step: find complexes relevant to the disease
    # gene names
    gene_disease <- names(y)[y == 1L] %>% unique
    # find their associated complexes
    df_compl <- filter(df_complex, STRING_id %in% gene_disease)
    # genes within the disease that belong to complexes
    gene_compl <- df_compl$STRING_id %>% unique
    # identifiers of the complexes related to the disease
    id_compl <- df_compl$complex_id %>% as.character %>% unique
    # have the complexes as a list
    list_whole_compl <- list_complex[id_compl]
    gene_whole_compl <- list_whole_compl %>% unlist %>% unique
    
    # Second step: generate graph with genes as nodes for merging complexes
    # 
    # union of disease genes and complex genes for that disease
    # (because some complex genes might not be within the disease - 
    # this should not happen too often though)
    gene_union <- union(gene_whole_compl, gene_disease)
    n <- length(gene_union)
    # First, an empty graph
    g <- graph.empty(n)
    V(g)$name <- gene_union
    # Modify its adjacency matrix by adding edges between all the 
    # genes in a complex
    mat_adj <- get.adjacency(g)
    for (com in list_whole_compl) {
      mat_adj[com, com] <- 1
    }
    # Redefine the graph with the right adjacency
    g <- graph_from_adjacency_matrix(mat_adj, mode = "undirected")
    g_components <- clusters(g)
    
    # Get the labels for each gene
    labels_gene <- g_components$membership
    labels <- 1:(g_components$no)
    # Number of genes in each component
    labels_size <- g_components$csize
    # Compute the number of disease genes per component
    # (because some of them might not be fully made of disease genes...)
    labels_positives <- sapply(
      labels, 
      function(lab) {
        genes <- names(labels_gene)[labels_gene == lab]
        length(intersect(genes, gene_disease))
      }
    )
    stopifnot(all(labels_size >= labels_positives))
    
    # ids of the rest of vertices
    genes_outside <- V(g_filter)[!(name %in% V(g)$name)] %>% as.numeric
    
    # names for repetitions (with trailing 0s if needed)
    names_rep <- paste0(
      "Rep", 
      formatC(1:config$times_cv, 
              width = ceiling(log10(config$times_cv)), 
              format = "d", flag = "0"))
    #######################################
    # Complex-aware 1st method: block CV
    # browser()
    # all the splits in a list
    # Format: list$RepX$FoldY and list$RepX.FoldY after unlisting
    ans$block <- plyr::llply(
      setNames(names_rep, names_rep), 
      function(rep_number) {
        # browser()
        # shuffle the components
        shuffle_labels <- sample(labels)
        # cumulative number of disease genes in this ordering
        shuffle_sum <- labels_positives[shuffle_labels]
        shuffle_cumsum <- cumsum(shuffle_sum)
        
        # Total number of disease genes
        total_sum <- length(gene_disease)
        stopifnot(total_sum == tail(shuffle_cumsum, 1))
        
        # Theoretical endpoints
        endpoints <- ((1:config$k_cv - 1)/config$k_cv)*total_sum
        # Find the closest cuts
        cuts <- sapply(endpoints, function(x) which.min(abs(shuffle_cumsum - x)))
        # In which cut is each position?
        # This leaves the positives (and maybe some negatives) with a defined fold
        folds_inside <- sapply(seq_along(shuffle_sum), function(x) sum(x >= cuts)) %>% 
          paste0("Fold", .) %>% 
          split(x = shuffle_labels, f = .) %>% 
          llply(function(labels_fold) {
            # get the ids of the nodes in each fold
            genes_fold <- V(g)[labels_gene %in% labels_fold]$name
            V(g_filter)[name %in% genes_fold] %>% as.numeric
          })
        
        # Assuming that the number of negatives in the complexes is small.. 
        # the remaining negatives will be equally split among folds
        folds_outside <- caret::createFolds(genes_outside, k = config$k_cv) %>% 
          llply(function(ids_fold) {
            genes_outside[ids_fold]
          })

        # Merge both lists and take the complementary, 
        # as each fold contains the training data.
        lapply(
          setNames(names(folds_inside), names(folds_inside)), 
          function(fold) {
            list(train = setdiff(
              1:vcount(g_filter), 
              c(folds_inside[[fold]], folds_outside[[fold]])
              )
            )
          }
        )
      }
    ) %>% unlist(recursive = FALSE, use.names = TRUE)
    
    #######################################
    # Complex-aware 2nd method: pick one representative per block
    ans$representative <- plyr::llply(
      setNames(names_rep, names_rep), 
      function(rep_number) {
        # for each merged complex, pick one representative
        # the rest of genes belong to "excluded"
        
        # representatives (by string id)
        # This will change between repetitions!
        list_repr <- lapply(
          labels, 
          function(lab) {
            # find genes and permute them
            genes <- names(labels_gene)[labels_gene == lab]
            genes_disease <- intersect(genes, gene_disease) %>% sample
            
            # pick the first gene as representative and the rest as excluded
            genes_disease[1]
          }
        ) %>% unlist
        
        # new vector of labels
        y_new <- setNames(V(g_filter)$name %in% list_repr, names(V(g_filter)))*1L
        
        # list of excluded genes
        list_excluded <- setdiff(V(g)$name, list_repr)
        id_excluded <- which(V(g_filter)$name %in% list_excluded)
        
        # browser()
        # stratified partition of the new training vector
        folds <- caret::createFolds(as.factor(y_new), 
                                    k = config$k_cv, 
                                    returnTrain = TRUE) 
        
        # return 
        list_folds <- lapply(
          setNames(names(folds), names(folds)), 
          function(fold) {
            list(
              train = setdiff(folds[[fold]], id_excluded), 
              validation = setdiff(1:n_total, c(folds[[fold]], id_excluded))
            )
          }
        )
        
        list_folds
      }
    ) %>% unlist(recursive = FALSE, use.names = TRUE)
    
    ans
  }, 
  .progress = "text"
)
```

## Sanity check of folds

```{r}
df_stat_complex <- plyr::ldply(
  setNames(names(list_cv_folds), names(list_cv_folds)), 
  function(disease_name) {
    # binary drugs data
    df_dis <- filter(df_input, disease == disease_name & 
                       input_type == "drugs" & score == 1L)
    vec_dis <- setNames(
      V(g_filter)$name %in% df_dis$STRING_id, 
      V(g_filter)$name)
    gene_dis <- names(vec_dis)[vec_dis == 1L]
    
    df_complex_in_disease <- filter(df_complex, STRING_id %in% gene_dis)
    
    # check all the cross validation schemes
    plyr::ldply(
      list_cv_folds[[disease_name]], 
      function(scheme) {
        # and all the folds within it
        plyr::ldply(
          scheme, 
          function(rep_fold) {
            # browser()
            
            id_train <- unique(rep_fold$train)
            id_val <- unique(rep_fold$validation)
            if(is.null(id_val)) id_val <- -id_train
            
            vec_train <- vec_dis[id_train]
            vec_val <- vec_dis[id_val]
            
            # breaks any complex?
            # see if all the complex genes involved with the disease
            # are inside or outside
            # the training set
            # i.e. either 0% or 100% of their genes map to it within 
            # the training set
            complex_mapped <- plyr::daply(
              df_complex_in_disease, 
              "complex_id", 
              function(df_comp) {
                # browser()
                genes_comp <- df_comp$STRING_id %>% as.character
                in_train <- any(genes_comp %in% names(vec_train))
                in_val <- any(genes_comp %in% names(vec_val))
                # complexes are split if and only if we have 
                # positives on training and on validation
                in_train & in_val
              }
            ) 
            # NAs are complexes that have no members 
            complex_split <- sum(complex_mapped, na.rm = TRUE)
            
            data.frame(
              train_all = length(vec_train), 
              train_pos = sum(vec_train), 
              validation_all = length(vec_val), 
              validation_pos = sum(vec_val), 
              split_complexes = complex_split
            )
          }, 
          .id = "fold"
        )
      }, 
      .id = "cv_scheme"
    )
  }, 
  .id = "disease", 
  .progress = "text"
)

# make sure there are always 
stopifnot(all(df_stat_complex$train_pos > 0))
stopifnot(all(df_stat_complex$train_val > 0))

# we can see that block cv discarded some folds
# due to empty train or val
table(df_stat_complex$cv_scheme)
summary(df_stat_complex)

# check that "classic" and "representative" are balanced
# we substract the maximum and the minimum amount of genes 
# in training and validation
# If this is 1 or less, it is balanced. 
# If not,
df_check <- dplyr::group_by(df_stat_complex, disease, cv_scheme) %>% 
  summarise(range_train = max(train_pos) - min(train_pos), 
            range_val = max(validation_pos) - min(validation_pos))

# Check balance (meaning that cv is stratified as expected)
stopifnot(all(filter(df_check, cv_scheme != "block")$range_train <= 1))
stopifnot(all(filter(df_check, cv_scheme != "block")$range_val <= 1))

# check number of split complexes by cv scheme
dplyr::group_by(df_stat_complex, cv_scheme) %>% 
  summarise(split_complexes_total = sum(split_complexes))


dplyr::group_by(df_stat_complex, disease, cv_scheme) %>% 
  summarise(npos_mean = mean(train_pos), 
            npos_sd = sd(train_pos))
```

```{r}
# Save CV schemes and summary data
save(list_cv_folds, 
     file = paste0(config$dir_performance, "/data_cv_splits.RData"), 
     compress = "xz")

write.csv(df_stat_complex, 
          file = paste0(config$dir_performance, "/descriptive_cv_splits.csv"), 
          row.names = FALSE)
```

# Heavy runs

```{r}
# Run only the cv schemes defined for this hostname 
# (see the 20_config file)
# 
# This is the core of the whole analysis
plyr::l_ply(
  config$cv_jobs, 
  function(cv_scheme) {
    # reproducibility
    set.seed(1)
    
    file_name <- paste0(config$dir_performance, 
                        "/metrics_cvscheme_", cv_scheme, ".csv")
    file_time <- paste0(config$dir_performance, 
                        "/time_cvscheme_", cv_scheme, ".txt")
    
    # write csv header
    header_csv <- c("disease", "input_type", "split_cv", 
                "method", names(list_metrics))
    write(header_csv, file = file_name, 
          ncolumns = length(header_csv), sep = "\t")
    
    # save the time invested in the run
    time_run <- system.time({
      plyr::d_ply(
        df_input,
        c("disease", "input_type"),
        function(df_in) {
          # browser()
          name_disease <- as.character(df_in$disease[1])
          name_input_type <- as.character(df_in$input_type[1])

          # reference streams
          df_streams_disease <- filter(df_streams, disease == name_disease)
          # cv folds
          list_cv <- list_cv_folds[[name_disease]][[cv_scheme]]

          # x: input
          # y: validation
          # Both are named vectors
          x <- setNames(df_in$score, df_in$STRING_id)
          y <- setNames(df_in$validation, df_in$STRING_id)

          # benchmark all the methods for this disease and cv_scheme
          list_perf <- plyr::l_ply(
            names(list_cv),
            function(split_cv_name) {
              # browser()
              ######## define training and validation ########
              # the next line contains the indices of the training instances
              split_cv <- list_cv[[split_cv_name]]

              # train vectors, with three formats
              # diffuStats + EGAD: positive 1, negative 0, unabelled NULL
              vec_diffustats <- x[split_cv$train]
              # COSnet: positive 1, negative -1, unlabelled 0
              vec_cosnet <- ifelse(x == 1, 1, 0)
              vec_cosnet[-split_cv$train] <- 0
              # treat negatives as unlabelled (see commented lines below)
              # vec_cosnet <- ifelse(x == 1, 1, -1)
              # vec_cosnet[-split_cv_train] <- 0

              # RANKS: which(1) - but only for training fold!
              vec_ranks <- which(vec_cosnet == 1)
              # EGAD: positive 1, negative/unlabelled 0
              vec_egad <- ifelse(vec_cosnet == 1, 1, 0)
              # debug
              # table(vec_diffustats)
              # table(vec_cosnet)
              # length(vec_ranks)
              # table(vec_egad)

              # validation labels
              if (is.null(split_cv$validation)) vec_val <- y[-split_cv$train]
              else vec_val <- y[split_cv$validation]
              # for safety, we will index all the results using the
              # names of the validation genes, making sure the order
              # is kept...
              names_train <- names(vec_diffustats)
              names_val <- names(vec_val)

              ######## diffusion-based approaches ########
              # diffuStats
              list_scores <- plyr::llply(
                setNames(config$list_methods, config$list_methods),
                function(method) {
                  diffuStats::diffuse(
                    K = K, scores = vec_diffustats,
                    method = method, n.perm = config$mc_nperm)[names_val]
                }
              )

              # personalized PageRank
              list_scores$ppr <- page.rank(
                g_filter, personalized = vec_egad)$vector[names_val]

              # EGAD (gba)
              list_scores$EGAD <- EGAD::predictions(
                genes.labels = vec_egad,
                network = A
              )[, 1]
              list_scores$EGAD <- list_scores$EGAD[names_val]

              # RANKS (wsld + knn) kernelized scores
              list_scores$wsld <- RANKS::WSLD.score(
                RW = K, x = 1:nrow(K), x.pos = vec_ranks, d = config$wsld_d) %>%
                setNames(rownames(K))
              list_scores$wsld <- list_scores$wsld[names_val]
              list_scores$knn <- RANKS::KNN.score(
                RW = K, x = 1:nrow(K), x.pos = vec_ranks, k = config$knn_k) %>%
                setNames(rownames(K))
              list_scores$knn <- list_scores$knn[names_val]

              ######## other machine learning approaches ########
              # based in prodige1: SVM
              list_scores$bagsvm <- bag_svm(
                ind_train = names_train, ind_test = names_val,
                K = K, ytrain = vec_diffustats, B = 30,
                C = 1, type = "C-svc", scaled = FALSE)
              # based in MashUp: SVM and RandomForest
              list_mashup <- mlr_svm_rf(
                ind_train = names_train, ind_test = names_val,
                df_features = df_mashup, ytrain = vec_diffustats)
              list_scores <- c(list_scores, list_mashup)

              # COSNet: neural net
              list_scores$COSNet <- COSNet::COSNet(
                W = A, labeling = vec_cosnet, cost = config$cosnet_cost
              )$scores
              list_scores$COSNet <- list_scores$COSNet[names_val]

              ######## reference scores ########
              # random
              list_scores$random <- setNames(
                sample(length(names_val)), names_val
              )
              # network properties that ignore input
              list_scores$randomraw <- diffuStats::diffuse(
                K = K,
                scores = setNames(sample(vec_diffustats),
                                  names(vec_diffustats)),
                method = "raw")[names_val]
              list_scores$pr <- pr[names_val]

              # reference: other data streams
              list_streams <- plyr::dlply(
                df_streams_disease,
                "stream",
                function(df) {
                  stream_scores <- setNames(df$score, df$STRING_id)
                  stream_scores[names_val]
                }
              )
              # browser()
              ######## performance metrics ########
              # compute metrics
              df_metrics <- plyr::ldply(
                c(list_scores, list_streams),
                function(scores) {
                  perf_eval(
                    prediction = scores,
                    validation = vec_val,
                    metric = list_metrics
                  )
                },
                .id = "method"
              )

              # Append to file, so that we can keep track on how the file grows
              # and how advanced the process is
              df_append <- data.frame(
                disease = name_disease,
                input_type = name_input_type,
                split_cv = split_cv_name,
                df_metrics
              )
              write.table(df_append, file_name, sep = "\t",
                          append = TRUE, row.names = FALSE, col.names = FALSE)
            }
          )
        }
      )
    })
    # save time metadata
    writeLines(
      capture.output(print(time_run)), 
      con = file_time
    )
  }
)
```

# Reproducibility

```{r}
out <- capture.output(sessionInfo())
writeLines(
  out, 
  con = paste0(config$dir_metadata, 
               "/22_sessionInfo_performance_host_", 
               config$host, ".txt"))
```

